{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA_Session_3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "### **Import Libraries and modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "outputId": "2ed4d688-de28-4618-9871-35ed0fa61420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqHhymamBXle",
        "colab_type": "text"
      },
      "source": [
        "###Analyzing mnist dataset \n",
        "To proceed further with the convoution, it is always necessary to have a better understanding of how the dataset looks and what could be the prediction dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "2c4a3b5b-4c91-4bdf-d631-b282783a3366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])\n",
        "plt.imshow(X_train[15])\n",
        "plt.imshow(X_train[500])\n",
        "plt.imshow(X_train[3000])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0a6cb9bac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E8ZL9z9DijO",
        "colab_type": "text"
      },
      "source": [
        "###Data Pre-Processing\n",
        "\n",
        "Now we need to reshape the images as the model expects. The last number 1 signifies that the image is in greyscale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny1tpyKVHUJt",
        "colab_type": "text"
      },
      "source": [
        "We also need to analyzing the Class array and 'one-hot-encode' them to so that our model could make more sense out of it. \n",
        "\n",
        "We can see the sample of class array below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "3a49d1f1-02b4-4109-a686-7ef052778941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-vlW_LhH-yA",
        "colab_type": "text"
      },
      "source": [
        "after processing, we can see that the value of 5 is replaced by an array which signifies its value by keeping 1 in the 6th place (array starts by index 0) and rest of the array as 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "ba67bdaa-56e9-4239-9778-c12767ced719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Y_train[:10]\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7iYzvebJDrl",
        "colab_type": "text"
      },
      "source": [
        "###Building the Model\n",
        "\n",
        "I have built a Sequential model, which allows us to add layers one upon other. We use model.add() to add layers. I am using 3x3 size of kernel for every convolution layer and 2x2 for Maxpooling layers.\n",
        "\n",
        "I have started with 2 convolution layers with 16 and added a maxpooling layer becasue at this point the receptive field is 5x5 and since the input size is only 24x24, applying maxpooling at this point will not take out too much information.\n",
        "\n",
        "i am doing another convolution followed by a maxpooling becasue I assume that after convolution, since the receptive field has alerady hit a 12x12, the parts of object is being identified. Further, I have used a 1x1 to integrate multiple feature maps into one channel and creating 10 such channels. Finally the Batch normalization and dropout has been used to get rid of the Ovrefitting issue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKqT73Q9JJB",
        "colab_type": "code",
        "outputId": "3ab792d0-09ef-41a2-b0a8-d5b0c9dabc9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "from keras.layers import Activation, BatchNormalization\n",
        "#from keras.layers.normalization import BatchNormalization\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Convolution2D(10, 1, activation='relu'))\n",
        "model.add(Convolution2D(10, 5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(.03))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 10, 10, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 5, 5, 10)          330       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 1, 1, 10)          2510      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 1, 1, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 16,928\n",
            "Trainable params: 16,908\n",
            "Non-trainable params: 20\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7AUNI9QHnKE",
        "colab_type": "text"
      },
      "source": [
        "###Compiling the model\n",
        "\n",
        "Here I am using *adam* optimizer, where I have also used the learning rate 0.002 an momentum as 0.9 so that the model converges slowly to the minima and we see less variance in the accuracy after each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp6SuGrL9M3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=keras.optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "#model.compile(loss='categorical_crossentropy',\n",
        " #            optimizer='adam',\n",
        "  #           metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNhc--VrIgNg",
        "colab_type": "text"
      },
      "source": [
        "###Training the model\n",
        "\n",
        "Finally we have arrived to the step where we are going to train our model. I have run the model with multiple batch sizes and found that it is performing better at *batch_size =64*. I have set the epochs at 85, only to see how ot performs in a long run. \n",
        "\n",
        "The *validation_data* parameter allows the model to perform accuracy test at the end of each epoch.\n",
        "\n",
        "####We have been successfully able to achieve > 99.40% accuracy on 27th Epoch.\n",
        "Though it is not the accuracy is varying through-out, we still have been able to cross >99.35% accuracy for a few times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xWoKhPY9Of5",
        "colab_type": "code",
        "outputId": "a7a79c81-1907-4ef0-e1cd-99bcb4a98df5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3046
        }
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=64, nb_epoch=85, verbose=1, validation_data=(X_test,Y_test))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/85\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.3475 - acc: 0.9316 - val_loss: 0.0876 - val_acc: 0.9848\n",
            "Epoch 2/85\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.1589 - acc: 0.9553 - val_loss: 0.0888 - val_acc: 0.9832\n",
            "Epoch 3/85\n",
            "60000/60000 [==============================] - 5s 90us/step - loss: 0.1215 - acc: 0.9633 - val_loss: 0.0394 - val_acc: 0.9901\n",
            "Epoch 4/85\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.1017 - acc: 0.9678 - val_loss: 0.0343 - val_acc: 0.9911\n",
            "Epoch 5/85\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.0889 - acc: 0.9723 - val_loss: 0.0363 - val_acc: 0.9910\n",
            "Epoch 6/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0750 - acc: 0.9768 - val_loss: 0.0317 - val_acc: 0.9909\n",
            "Epoch 7/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0700 - acc: 0.9784 - val_loss: 0.0384 - val_acc: 0.9891\n",
            "Epoch 8/85\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0631 - acc: 0.9815 - val_loss: 0.0276 - val_acc: 0.9932\n",
            "Epoch 9/85\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.0567 - acc: 0.9828 - val_loss: 0.0255 - val_acc: 0.9931\n",
            "Epoch 10/85\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0518 - acc: 0.9842 - val_loss: 0.0280 - val_acc: 0.9905\n",
            "Epoch 11/85\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0497 - acc: 0.9844 - val_loss: 0.0278 - val_acc: 0.9921\n",
            "Epoch 12/85\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0464 - acc: 0.9859 - val_loss: 0.0236 - val_acc: 0.9925\n",
            "Epoch 13/85\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0447 - acc: 0.9864 - val_loss: 0.0268 - val_acc: 0.9924\n",
            "Epoch 14/85\n",
            "60000/60000 [==============================] - 5s 90us/step - loss: 0.0411 - acc: 0.9870 - val_loss: 0.0245 - val_acc: 0.9923\n",
            "Epoch 15/85\n",
            "60000/60000 [==============================] - 5s 90us/step - loss: 0.0397 - acc: 0.9872 - val_loss: 0.0278 - val_acc: 0.9910\n",
            "Epoch 16/85\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0378 - acc: 0.9875 - val_loss: 0.0220 - val_acc: 0.9926\n",
            "Epoch 17/85\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.0371 - acc: 0.9876 - val_loss: 0.0243 - val_acc: 0.9926\n",
            "Epoch 18/85\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.0341 - acc: 0.9886 - val_loss: 0.0286 - val_acc: 0.9911\n",
            "Epoch 19/85\n",
            "60000/60000 [==============================] - 5s 90us/step - loss: 0.0344 - acc: 0.9888 - val_loss: 0.0282 - val_acc: 0.9918\n",
            "Epoch 20/85\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0336 - acc: 0.9883 - val_loss: 0.0222 - val_acc: 0.9928\n",
            "Epoch 21/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0304 - acc: 0.9898 - val_loss: 0.0294 - val_acc: 0.9915\n",
            "Epoch 22/85\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0300 - acc: 0.9900 - val_loss: 0.0250 - val_acc: 0.9924\n",
            "Epoch 23/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0310 - acc: 0.9894 - val_loss: 0.0274 - val_acc: 0.9916\n",
            "Epoch 24/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0281 - acc: 0.9904 - val_loss: 0.0226 - val_acc: 0.9930\n",
            "Epoch 25/85\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0282 - acc: 0.9905 - val_loss: 0.0304 - val_acc: 0.9910\n",
            "Epoch 26/85\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.0263 - acc: 0.9906 - val_loss: 0.0266 - val_acc: 0.9918\n",
            "Epoch 27/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0268 - acc: 0.9907 - val_loss: 0.0223 - val_acc: 0.9941\n",
            "Epoch 28/85\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0242 - acc: 0.9914 - val_loss: 0.0224 - val_acc: 0.9939\n",
            "Epoch 29/85\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0272 - acc: 0.9900 - val_loss: 0.0265 - val_acc: 0.9912\n",
            "Epoch 30/85\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0241 - acc: 0.9914 - val_loss: 0.0325 - val_acc: 0.9912\n",
            "Epoch 31/85\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0236 - acc: 0.9918 - val_loss: 0.0255 - val_acc: 0.9924\n",
            "Epoch 32/85\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0244 - acc: 0.9908 - val_loss: 0.0246 - val_acc: 0.9929\n",
            "Epoch 33/85\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.0246 - acc: 0.9909 - val_loss: 0.0238 - val_acc: 0.9923\n",
            "Epoch 34/85\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0229 - acc: 0.9912 - val_loss: 0.0278 - val_acc: 0.9923\n",
            "Epoch 35/85\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0220 - acc: 0.9917 - val_loss: 0.0277 - val_acc: 0.9919\n",
            "Epoch 36/85\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0229 - acc: 0.9912 - val_loss: 0.0269 - val_acc: 0.9917\n",
            "Epoch 37/85\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0220 - acc: 0.9917 - val_loss: 0.0264 - val_acc: 0.9917\n",
            "Epoch 38/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0224 - acc: 0.9913 - val_loss: 0.0242 - val_acc: 0.9933\n",
            "Epoch 39/85\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0217 - acc: 0.9912 - val_loss: 0.0240 - val_acc: 0.9938\n",
            "Epoch 40/85\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0209 - acc: 0.9917 - val_loss: 0.0235 - val_acc: 0.9936\n",
            "Epoch 41/85\n",
            "60000/60000 [==============================] - 5s 91us/step - loss: 0.0208 - acc: 0.9921 - val_loss: 0.0295 - val_acc: 0.9920\n",
            "Epoch 42/85\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0203 - acc: 0.9925 - val_loss: 0.0267 - val_acc: 0.9926\n",
            "Epoch 43/85\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0207 - acc: 0.9920 - val_loss: 0.0261 - val_acc: 0.9925\n",
            "Epoch 44/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0191 - acc: 0.9927 - val_loss: 0.0314 - val_acc: 0.9906\n",
            "Epoch 45/85\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0206 - acc: 0.9917 - val_loss: 0.0276 - val_acc: 0.9927\n",
            "Epoch 46/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0184 - acc: 0.9929 - val_loss: 0.0300 - val_acc: 0.9920\n",
            "Epoch 47/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0201 - acc: 0.9922 - val_loss: 0.0243 - val_acc: 0.9929\n",
            "Epoch 48/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0202 - acc: 0.9920 - val_loss: 0.0246 - val_acc: 0.9925\n",
            "Epoch 49/85\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0189 - acc: 0.9922 - val_loss: 0.0251 - val_acc: 0.9931\n",
            "Epoch 50/85\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0168 - acc: 0.9933 - val_loss: 0.0274 - val_acc: 0.9924\n",
            "Epoch 51/85\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0179 - acc: 0.9930 - val_loss: 0.0294 - val_acc: 0.9922\n",
            "Epoch 52/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0186 - acc: 0.9924 - val_loss: 0.0336 - val_acc: 0.9901\n",
            "Epoch 53/85\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0176 - acc: 0.9928 - val_loss: 0.0248 - val_acc: 0.9927\n",
            "Epoch 54/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0192 - acc: 0.9918 - val_loss: 0.0247 - val_acc: 0.9932\n",
            "Epoch 55/85\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0187 - acc: 0.9927 - val_loss: 0.0249 - val_acc: 0.9924\n",
            "Epoch 56/85\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0174 - acc: 0.9930 - val_loss: 0.0266 - val_acc: 0.9921\n",
            "Epoch 57/85\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0166 - acc: 0.9935 - val_loss: 0.0275 - val_acc: 0.9926\n",
            "Epoch 58/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0188 - acc: 0.9924 - val_loss: 0.0262 - val_acc: 0.9937\n",
            "Epoch 59/85\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0173 - acc: 0.9929 - val_loss: 0.0276 - val_acc: 0.9929\n",
            "Epoch 60/85\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0160 - acc: 0.9934 - val_loss: 0.0276 - val_acc: 0.9919\n",
            "Epoch 61/85\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.0152 - acc: 0.9938 - val_loss: 0.0282 - val_acc: 0.9920\n",
            "Epoch 62/85\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.0178 - acc: 0.9927 - val_loss: 0.0274 - val_acc: 0.9924\n",
            "Epoch 63/85\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.0171 - acc: 0.9931 - val_loss: 0.0269 - val_acc: 0.9928\n",
            "Epoch 64/85\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.0158 - acc: 0.9932 - val_loss: 0.0273 - val_acc: 0.9925\n",
            "Epoch 65/85\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0152 - acc: 0.9938 - val_loss: 0.0318 - val_acc: 0.9916\n",
            "Epoch 66/85\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.0166 - acc: 0.9931 - val_loss: 0.0248 - val_acc: 0.9925\n",
            "Epoch 67/85\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.0159 - acc: 0.9933 - val_loss: 0.0263 - val_acc: 0.9931\n",
            "Epoch 68/85\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.0165 - acc: 0.9930 - val_loss: 0.0275 - val_acc: 0.9926\n",
            "Epoch 69/85\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0160 - acc: 0.9934 - val_loss: 0.0273 - val_acc: 0.9929\n",
            "Epoch 70/85\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.0168 - acc: 0.9924 - val_loss: 0.0280 - val_acc: 0.9925\n",
            "Epoch 71/85\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0161 - acc: 0.9933 - val_loss: 0.0279 - val_acc: 0.9926\n",
            "Epoch 72/85\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.0162 - acc: 0.9930 - val_loss: 0.0282 - val_acc: 0.9926\n",
            "Epoch 73/85\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0151 - acc: 0.9938 - val_loss: 0.0277 - val_acc: 0.9919\n",
            "Epoch 74/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0152 - acc: 0.9933 - val_loss: 0.0356 - val_acc: 0.9908\n",
            "Epoch 75/85\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.0151 - acc: 0.9934 - val_loss: 0.0276 - val_acc: 0.9923\n",
            "Epoch 76/85\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0148 - acc: 0.9937 - val_loss: 0.0255 - val_acc: 0.9931\n",
            "Epoch 77/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0142 - acc: 0.9938 - val_loss: 0.0329 - val_acc: 0.9914\n",
            "Epoch 78/85\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0156 - acc: 0.9934 - val_loss: 0.0263 - val_acc: 0.9928\n",
            "Epoch 79/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0148 - acc: 0.9934 - val_loss: 0.0303 - val_acc: 0.9914\n",
            "Epoch 80/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0157 - acc: 0.9933 - val_loss: 0.0288 - val_acc: 0.9914\n",
            "Epoch 81/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0159 - acc: 0.9932 - val_loss: 0.0245 - val_acc: 0.9939\n",
            "Epoch 82/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0143 - acc: 0.9936 - val_loss: 0.0307 - val_acc: 0.9910\n",
            "Epoch 83/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0144 - acc: 0.9939 - val_loss: 0.0262 - val_acc: 0.9927\n",
            "Epoch 84/85\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0154 - acc: 0.9934 - val_loss: 0.0325 - val_acc: 0.9919\n",
            "Epoch 85/85\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0148 - acc: 0.9934 - val_loss: 0.0307 - val_acc: 0.9917\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0a6cb382b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsH-lLk-eLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkX8JMv79q9r",
        "colab_type": "code",
        "outputId": "0149d2d5-d90a-4bd4-be05-8f1cf1f528df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.030660102597329025, 0.9917]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "outputId": "21f18cb8-ffc4-41fe-8f1a-24be490cd1ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(y_test[:9])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4.78894435e-09 5.83403637e-09 9.93843088e-08 1.02689768e-08\n",
            "  1.85303470e-08 1.07293663e-09 1.44160976e-08 9.99999881e-01\n",
            "  4.02884588e-08 9.52809565e-09]\n",
            " [7.64183994e-10 1.14436782e-09 9.99999762e-01 2.27464936e-09\n",
            "  7.37441608e-10 1.81570926e-17 1.94169786e-07 3.05804937e-10\n",
            "  2.69756073e-14 4.57625965e-10]\n",
            " [6.85029722e-09 9.99999285e-01 1.20323262e-08 2.48999807e-08\n",
            "  4.20052828e-08 1.75027012e-07 6.55154295e-08 4.02854390e-08\n",
            "  8.60768665e-08 2.90699859e-07]\n",
            " [9.99994993e-01 2.18742358e-07 1.32834157e-06 6.23558037e-07\n",
            "  7.03541545e-08 7.53136220e-08 2.58967621e-08 5.06596081e-08\n",
            "  1.34708387e-07 2.53761414e-06]\n",
            " [3.49100264e-06 6.75417322e-09 1.85358815e-07 2.41103589e-06\n",
            "  9.99993682e-01 8.22367099e-08 4.52322375e-08 1.11069856e-08\n",
            "  8.60611138e-08 7.73102471e-08]\n",
            " [8.26702120e-08 9.99996543e-01 2.09632006e-07 1.00861676e-07\n",
            "  6.58524186e-07 1.88860980e-07 1.06239418e-06 6.42790553e-07\n",
            "  1.68612956e-07 3.18206816e-07]\n",
            " [2.02442663e-11 5.37322933e-08 2.20749953e-05 1.21077992e-09\n",
            "  9.99969959e-01 4.82975153e-08 6.29938768e-06 3.48133414e-12\n",
            "  1.58038358e-06 6.29041530e-09]\n",
            " [1.65188947e-08 1.32928704e-07 1.34881893e-05 1.97379094e-07\n",
            "  3.33024241e-06 1.18428245e-08 3.45115536e-09 1.86611371e-09\n",
            "  5.38149322e-07 9.99982238e-01]\n",
            " [7.10759139e-07 2.34801298e-08 1.27691337e-07 2.55163819e-08\n",
            "  9.52113680e-07 3.32241237e-01 6.67736709e-01 4.85432793e-06\n",
            "  1.53292895e-05 1.40201175e-08]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT--y98_dr2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GY4Upv4dsUR",
        "colab_type": "code",
        "outputId": "9fc1d625-8ae9-4ff2-8cfa-09a313e4669c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "%matplotlib inline\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    #x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n",
        "                      layer_name = 'conv2d_14'):\n",
        "    layer_output = layer_dict[layer_name].output\n",
        "    img_ascs = list()\n",
        "    for filter_index in range(layer_output.shape[3]):\n",
        "        # build a loss function that maximizes the activation\n",
        "        # of the nth filter of the layer considered\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "        # step size for gradient ascent\n",
        "        step = 5.\n",
        "\n",
        "        img_asc = np.array(img)\n",
        "        # run gradient ascent for 20 steps\n",
        "        for i in range(20):\n",
        "            loss_value, grads_value = iterate([img_asc])\n",
        "            img_asc += grads_value * step\n",
        "\n",
        "        img_asc = img_asc[0]\n",
        "        img_ascs.append(deprocess_image(img_asc).reshape((28, 28)))\n",
        "        \n",
        "    if layer_output.shape[3] >= 35:\n",
        "        plot_x, plot_y = 6, 6\n",
        "    elif layer_output.shape[3] >= 23:\n",
        "        plot_x, plot_y = 4, 6\n",
        "    elif layer_output.shape[3] >= 11:\n",
        "        plot_x, plot_y = 2, 6\n",
        "    else:\n",
        "        plot_x, plot_y = 1, 2\n",
        "    fig, ax = plt.subplots(plot_x, plot_y, figsize = (12, 12))\n",
        "    ax[0, 0].imshow(img.reshape((28, 28)), cmap = 'gray')\n",
        "    ax[0, 0].set_title('Input image')\n",
        "    fig.suptitle('Input image and %s filters' % (layer_name,))\n",
        "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
        "    for (x, y) in [(i, j) for i in range(plot_x) for j in range(plot_y)]:\n",
        "        if x == 0 and y == 0:\n",
        "            continue\n",
        "        ax[x, y].imshow(img_ascs[x * plot_y + y - 1], cmap = 'gray')\n",
        "        ax[x, y].set_title('filter %d' % (x * plot_y + y - 1))\n",
        "\n",
        "vis_img_in_filter()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-19229f66b51a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filter %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mplot_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mvis_img_in_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-19229f66b51a>\u001b[0m in \u001b[0;36mvis_img_in_filter\u001b[0;34m(img, layer_name)\u001b[0m\n\u001b[1;32m     22\u001b[0m def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n\u001b[1;32m     23\u001b[0m                       layer_name = 'conv2d_14'):\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mimg_ascs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilter_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'conv2d_14'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tvptcn8dxvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}